/**
 * (C) Copyright 2019 Intel Corporation.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * GOVERNMENT LICENSE RIGHTS-OPEN SOURCE SOFTWARE
 * The Government's rights to use, modify, reproduce, release, perform, display,
 * or disclose this software are subject to the terms of the Apache License as
 * provided in Contract No. B609815.
 * Any reproduction of computer software, computer software documentation, or
 * portions thereof marked with this legend must also reproduce the markings.
 */
/**
  * Implementation for aggregation and discard
 */
#define D_LOGFAC	DD_FAC(vos)

#include <daos_srv/vos.h>
#include "vos_internal.h"

#define AGG_CREDITS_MAX		10000
/*
 * EV tree iterator returns all logical entries in extent start order, and
 * the physical entry information is included in each logical entry as well.
 *
 * As the iterator moves on, the physical entries will be queued to form a
 * merge window, and when the window size reaches certain threshold, a merge
 * window flush will be triggered to replace the physical entries within the
 * window by the new coalesced physical entries.
 *
 * The visible logical entries will also be queued to facilitate data transfer
 * on merge window flush.
 *
 * Please be aware that EV tree sorted iterator is working on an out-of-band
 * auxiliary index, so it won't be affected by the in-tree operations on merge
 * window flush.
 */

/*
 * Threshold for merge window flush: When merge window size exceeded
 * AGG_MW_THRESH_FLUSH, merge window will stop growing and flush will be
 * triggered immediately.
 */
#define AGG_MW_THRESH_FLUSH	(1UL << 23)	/* 8MB */

/* EV tree physical entry */
struct agg_phy_ent {
	d_list_t		pe_link;
	/* Original in-tree rectangle */
	struct evt_rect		pe_rect;
	/* Entry payload address */
	bio_addr_t		pe_addr;
	/*
	 * Extent start offset for the truncated physical entry. (Entry
	 * straddles window end could be truncated on merge window flush)
	 */
	daos_off_t		pe_off;
	/* Pool map version of entry */
	uint32_t		pe_ver;
	/* Reference count */
	uint32_t		pe_ref;
};

/* EV tree logical entry */
struct agg_lgc_ent {
	struct evt_extent	 le_ext;
	struct agg_phy_ent	*le_phy_ent;
};

/*
 * EV tree logical segment (no holes), it'll be used to form new physical
 * rectangle and being inserted in evtree on merge window flush.
 */
struct agg_lgc_seg {
	/* Start index in mw_lgc_ents */
	unsigned int		 ls_idx_start;
	/* End index in mw_lgc_ents */
	unsigned int		 ls_idx_end;
	/* New segment generated by truncate on window flush */
	struct agg_phy_ent	*ls_phy_ent;
	/* Description of the new physical entry to be inserted */
	struct evt_entry_in	 ls_ent_in;
};

/* I/O context used on EV tree merge window flush */
struct agg_io_context {
	/* Temporary buffer for data transfer */
	void			*ic_buf;
	unsigned int		 ic_buf_len;
	/* Segments being involved on merge window flush */
	struct agg_lgc_seg	*ic_segs;
	unsigned int		 ic_seg_max;
	unsigned int		 ic_seg_cnt;
	/* Reserved SCM extents for new physical entries */
	struct pobj_action	*ic_scm_exts;
	unsigned int		 ic_scm_max;
	unsigned int		 ic_scm_cnt;
	/* Reserved NVMe extents for new physical entries */
	d_list_t		 ic_nvme_exts;
};

/* Merge window for evtree aggregation */
struct agg_merge_window {
	/* Record size */
	daos_size_t		 mw_rsize;
	/* Merge window extent */
	struct evt_extent	 mw_ext;
	/* Physical entries in merge window */
	d_list_t		 mw_phy_ents;
	unsigned int		 mw_phy_cnt;
	/* Visible logical entries in merge window */
	struct agg_lgc_ent	*mw_lgc_ents;
	unsigned int		 mw_lgc_max;
	unsigned int		 mw_lgc_cnt;
	/* I/O context for transfering data on flush */
	struct agg_io_context	 mw_io_ctxt;
};

struct vos_agg_param {
	uuid_t		ap_cookie;	/* update cookie, used by discard */
	uint32_t	ap_credits_max; /* # of tight loops to yield */
	uint32_t	ap_credits;	/* # of tight loops */
	bool		ap_discard;	/* discard or not */
	unsigned int	ap_sub_tree_empty:1,
			ap_sv_tree_empty:1,
			ap_ev_tree_empty:1;
	struct umem_instance	*ap_umm;
	/* SV tree: Max epoch in specified iterate epoch range */
	daos_epoch_t		 ap_max_epoch;
	/* EV tree: Merge window for evtree aggregation */
	struct agg_merge_window	 ap_window;
};

static inline void
mark_yield(bio_addr_t *addr, unsigned int *acts)
{
	if (addr->ba_type == DAOS_MEDIA_NVME)
		*acts |= VOS_ITER_CB_YIELD;
}

static int
agg_del_entry(daos_handle_t ih, struct umem_instance *umm,
	      vos_iter_entry_t *entry, unsigned int *acts)
{
	int	rc;

	D_ASSERT(umm != NULL);
	D_ASSERT(acts != NULL);

	rc = umem_tx_begin(umm, NULL);
	if (rc)
		return rc;

	mark_yield(&entry->ie_biov.bi_addr, acts);

	rc = vos_iter_delete(ih, NULL);
	if (rc != 0)
		rc = umem_tx_abort(umm, rc);
	else
		rc = umem_tx_commit(umm);

	if (rc) {
		D_ERROR("Failed to delete entry: %d\n", rc);
		return rc;
	}

	*acts |= VOS_ITER_CB_DELETE;
	return rc;
}

static int
agg_discard_parent(daos_handle_t ih, vos_iter_entry_t *entry,
		   struct vos_agg_param *agg_param, unsigned int *acts)
{
	int	rc;

	D_ASSERT(agg_param && agg_param->ap_discard);
	D_ASSERT(acts != NULL);

	if (!agg_param->ap_sub_tree_empty)
		return 0;

	/*
	 * All entries in sub-tree were deleted during the nested sub-tree
	 * iteration, then vos_iterate() re-probed the key in outer iteration
	 * to delete it.
	 *
	 * Since there can be at most 1 discard/aggregation ULT for each
	 * container at any given time, the key won't be deleted by others even
	 * if current ULT yield in sub-tree iteration, and re-probe will find
	 * the exact matched key.
	 */
	agg_param->ap_sub_tree_empty = 0;
	rc = agg_del_entry(ih, agg_param->ap_umm, entry, acts);
	if (rc) {
		D_ERROR("Failed to delete key entry: %d\n", rc);
	} else if (vos_iter_empty(ih) == 1) {
		agg_param->ap_sub_tree_empty = 1;
		/* Trigger re-probe in outer iteration */
		*acts |= VOS_ITER_CB_YIELD;
	}

	return rc;
}

static int
vos_agg_obj(daos_handle_t ih, vos_iter_entry_t *entry,
	    struct vos_agg_param *agg_param, unsigned int *acts)
{
	int	rc;

	D_ASSERT(agg_param != NULL);

	if (agg_param->ap_discard) {
		rc = agg_discard_parent(ih, entry, agg_param, acts);
		agg_param->ap_sub_tree_empty = 0;
		return rc;
	}

	return 0;
}

static int
vos_agg_dkey(daos_handle_t ih, vos_iter_entry_t *entry,
	     struct vos_agg_param *agg_param, unsigned int *acts)
{
	D_ASSERT(agg_param != NULL);

	if (agg_param->ap_discard)
		return agg_discard_parent(ih, entry, agg_param, acts);

	return 0;
}

static inline bool
ext1_covers_ext2(struct evt_extent *ext1, struct evt_extent *ext2)
{
	D_ASSERT(ext1->ex_lo <= ext1->ex_hi);
	D_ASSERT(ext2->ex_lo <= ext2->ex_hi);

	return (ext1->ex_lo <= ext2->ex_lo && ext1->ex_hi >= ext2->ex_hi);
}

enum {
	MW_CLOSED	= 0,	/* Resource released */
	MW_EMPTY,		/* No entries in window */
	MW_NONEMPTY,		/* Entries in window */
};

static int
merge_window_status(struct agg_merge_window *mw)
{
	struct agg_io_context	*io = &mw->mw_io_ctxt;

	D_ASSERT(io->ic_seg_cnt == 0);
	D_ASSERT(io->ic_scm_cnt == 0);
	D_ASSERT(d_list_empty(&io->ic_nvme_exts));

	D_ASSERT(mw->mw_ext.ex_lo <= mw->mw_ext.ex_hi);

	if (mw->mw_lgc_cnt != 0) {
		D_ASSERT(mw->mw_rsize != 0);
		D_ASSERT(mw->mw_phy_cnt != 0);
		D_ASSERT(!d_list_empty(&mw->mw_phy_ents));

		return MW_NONEMPTY;
	}

	D_ASSERT(!mw->mw_ext.ex_lo && !mw->mw_ext.ex_hi);

	if (mw->mw_lgc_ents != NULL) {
		/*
		 * Even there if there isn't any logical entries,
		 * there could be some truncated physical entries.
		 */
		D_ASSERT(mw->mw_rsize != 0);
		return MW_EMPTY;
	}

	/* Window closed, all resource should have been released */
	D_ASSERT(mw->mw_phy_cnt == 0);
	D_ASSERT(d_list_empty(&mw->mw_phy_ents));
	D_ASSERT(mw->mw_rsize == 0);
	D_ASSERT(mw->mw_lgc_max == 0);
	D_ASSERT(io->ic_buf == NULL);
	D_ASSERT(io->ic_buf_len == 0);

	return MW_CLOSED;
}

static int
vos_agg_akey(daos_handle_t ih, vos_iter_entry_t *entry,
	     struct vos_agg_param *agg_param, unsigned int *acts)
{
	D_ASSERT(agg_param != NULL);

	if (agg_param->ap_discard) {
		if (agg_param->ap_sv_tree_empty && agg_param->ap_ev_tree_empty)
			agg_param->ap_sub_tree_empty = 1;

		agg_param->ap_sv_tree_empty = 0;
		agg_param->ap_ev_tree_empty = 0;

		if (!agg_param->ap_sub_tree_empty)
			return 0;

		return agg_discard_parent(ih, entry, agg_param, acts);
	}

	/* Reset the max epoch for low-level SV tree iteration */
	agg_param->ap_max_epoch = 0;
	if (merge_window_status(&agg_param->ap_window) != MW_CLOSED)
		D_ASSERTF(false, "Merge window isn't closed.\n");

	return 0;
}

static int
vos_agg_sv(daos_handle_t ih, vos_iter_entry_t *entry,
	   struct vos_agg_param *agg_param, unsigned int *acts)
{
	int	rc;

	D_ASSERT(agg_param != NULL);
	D_ASSERT(entry->ie_epoch != 0);

	/* Discard: only delete recx with matched update cookie */
	if (agg_param->ap_discard) {
		if (uuid_compare(entry->ie_cookie, agg_param->ap_cookie))
			return 0;
		goto delete;
	}

	/* Aggregate: preserve the first recx which has highest epoch */
	if (agg_param->ap_max_epoch == 0) {
		agg_param->ap_max_epoch = entry->ie_epoch;
		return 0;
	}

	D_ASSERTF(entry->ie_epoch < agg_param->ap_max_epoch,
		  "max:"DF_U64", cur:"DF_U64"\n",
		  agg_param->ap_max_epoch, entry->ie_epoch);

delete:
	rc = agg_del_entry(ih, agg_param->ap_umm, entry, acts);
	if (rc) {
		D_ERROR("Failed to delete SV entry: %d\n", rc);
	} else if (vos_iter_empty(ih) == 1 && agg_param->ap_discard) {
		agg_param->ap_sv_tree_empty = 1;
		/* Trigger re-probe in akey iteration */
		*acts |= VOS_ITER_CB_YIELD;
	}

	return rc;
}

static int
prepare_segments(struct agg_merge_window *mw)
{
	struct agg_io_context	*io = &mw->mw_io_ctxt;
	struct agg_phy_ent	*phy_ent;
	struct agg_lgc_ent	*lgc_ent;
	struct agg_lgc_seg	*lgc_seg;
	struct evt_entry_in	*ent_in;
	struct evt_extent	 ext;
	unsigned int		 i;
	bool			 hole = true;

	/*
	 * Allocate large enough segments array to hold all the coalesced
	 * segments (at most mw_lgc_cnt) and truncated segments (at most
	 * mw_phy_cnt).
	 */
	D_ASSERT(mw->mw_lgc_cnt > 0);
	D_ASSERT(mw->mw_phy_cnt > 1);
	if ((mw->mw_lgc_cnt + mw->mw_phy_cnt) > io->ic_seg_max) {
		D_REALLOC(lgc_seg, io->ic_segs,
			  (mw->mw_lgc_cnt + mw->mw_phy_cnt) * sizeof(*lgc_seg));
		if (lgc_seg == NULL)
			return -DER_NOMEM;

		io->ic_segs = lgc_seg;
		io->ic_seg_max = (mw->mw_lgc_cnt + mw->mw_phy_cnt);
	}
	io->ic_seg_cnt = 0;

	/* Generate coalesced segments according to visible logical entries */
	for (i = 0; i < mw->mw_lgc_cnt; i++) {

		lgc_ent = &mw->mw_lgc_ents[i];
		phy_ent = lgc_ent->le_phy_ent;
		lgc_seg = &io->ic_segs[io->ic_seg_cnt];
		ent_in = &lgc_seg->ls_ent_in;

		ext = lgc_ent->le_ext;
		D_ASSERT(ext1_covers_ext2(&mw->mw_ext, &ext));

		if (bio_addr_is_hole(&phy_ent->pe_addr)) {
			io->ic_seg_cnt++;
			D_ASSERT(io->ic_seg_cnt < io->ic_seg_max);
			hole = true;
		} else {
			if (hole) {
				lgc_seg->ls_phy_ent = NULL;
				lgc_seg->ls_idx_start = i;
				ent_in->ei_inob = mw->mw_rsize;
				ent_in->ei_rect.rc_ex.ex_lo = ext.ex_lo;
			} else {
				D_ASSERT(ext.ex_lo ==
					 ent_in->ei_rect.rc_ex.ex_hi + 1);
			}

			lgc_seg->ls_idx_end = i;
			ent_in->ei_rect.rc_ex.ex_hi = ext.ex_hi;
			/* Merge to highest epoch */
			if (ent_in->ei_rect.rc_epc < phy_ent->pe_rect.rc_epc)
				ent_in->ei_rect.rc_epc =
					phy_ent->pe_rect.rc_epc;
			/* Merge to highest pool map version */
			if (ent_in->ei_ver < phy_ent->pe_ver)
				ent_in->ei_ver = phy_ent->pe_ver;
			hole = false;
		}
	}

	if (!hole)
		io->ic_seg_cnt++;
	D_ASSERT(io->ic_seg_cnt < io->ic_seg_max);

	/* Generate truncated segments according to physical entries */
	d_list_for_each_entry(phy_ent, &mw->mw_phy_ents, pe_link) {

		lgc_seg = &io->ic_segs[io->ic_seg_cnt];
		ent_in = &lgc_seg->ls_ent_in;

		ext = phy_ent->pe_rect.rc_ex;

		/* The physical entry was truncated on prev window flush */
		if (phy_ent->pe_off != 0)
			ext.ex_lo += phy_ent->pe_off;

		/* Physical entry must be in window or straddle window end */
		D_ASSERT(ext.ex_lo <= ext.ex_hi);
		D_ASSERT(ext.ex_lo >= mw->mw_ext.ex_lo);
		D_ASSERT(ext.ex_lo <= mw->mw_ext.ex_hi);

		/* Physical entry is in window */
		if (ext.ex_hi <= mw->mw_ext.ex_hi)
			continue;

		lgc_seg->ls_phy_ent = phy_ent;
		lgc_seg->ls_idx_start = 0;
		lgc_seg->ls_idx_end = 0;

		ent_in->ei_inob = mw->mw_rsize;
		ent_in->ei_rect.rc_ex.ex_lo = mw->mw_ext.ex_hi + 1;
		ent_in->ei_rect.rc_ex.ex_hi = ext.ex_hi;
		ent_in->ei_rect.rc_epc = phy_ent->pe_rect.rc_epc;
		ent_in->ei_ver = phy_ent->pe_ver;

		io->ic_seg_cnt++;
		D_ASSERT(io->ic_seg_cnt < io->ic_seg_max);
	}

	return 0;
}

static int
reserve_segment(struct vos_object *obj, struct agg_io_context *io,
		daos_size_t size, bio_addr_t *addr)
{
	struct vea_space_info	*vsi;
	struct vea_hint_context	*hint_ctxt;
	struct vea_resrvd_ext	*nvme_ext;
	uint32_t		 blk_cnt;
	uint16_t		 media;
	int			 rc;

	memset(addr, 0, sizeof(*addr));
	media = vos_media_select(obj, DAOS_IOD_ARRAY, size);

	if (media == DAOS_MEDIA_SCM) {
		struct pobj_action	*scm_ext;
		umem_id_t		 mmid;

		D_ASSERT(io->ic_scm_max > io->ic_scm_cnt);
		D_ASSERT(io->ic_scm_exts != NULL);
		scm_ext = &io->ic_scm_exts[io->ic_scm_cnt];

		mmid = umem_reserve(vos_obj2umm(obj), scm_ext, size);
		if (UMMID_IS_NULL(mmid)) {
			D_ERROR("Reserve "DF_U64" bytes on SCM failed.\n",
				size);
			return -DER_NOSPACE;
		}

		io->ic_scm_cnt++;
		bio_addr_set(addr, media, mmid.off);
		return 0;
	}

	D_ASSERT(media == DAOS_MEDIA_NVME);

	vsi = obj->obj_cont->vc_pool->vp_vea_info;
	D_ASSERT(vsi);
	hint_ctxt = obj->obj_cont->vc_hint_ctxt;
	D_ASSERT(hint_ctxt);
	blk_cnt = vos_byte2blkcnt(size);

	rc = vea_reserve(vsi, blk_cnt, hint_ctxt, &io->ic_nvme_exts);
	if (rc) {
		D_ERROR("Reserve %u blocks on NVMe failed: %d\n", blk_cnt, rc);
		return rc;
	}

	nvme_ext = d_list_entry(io->ic_nvme_exts.prev, struct vea_resrvd_ext,
				vre_link);
	D_ASSERTF(nvme_ext->vre_blk_cnt == blk_cnt, "%u != %u\n",
		  nvme_ext->vre_blk_cnt, blk_cnt);
	D_ASSERT(nvme_ext->vre_blk_off != 0);

	bio_addr_set(addr, media, nvme_ext->vre_blk_off << VOS_BLK_SHIFT);
	return 0;
}

static int
fill_one_segment(daos_handle_t ih, struct agg_merge_window *mw,
		 struct agg_lgc_seg *lgc_seg, unsigned int *acts)
{
	struct vos_obj_iter	*oiter = vos_hdl2oiter(ih);
	struct vos_object	*obj = oiter->it_obj;
	struct agg_io_context	*io = &mw->mw_io_ctxt;
	struct evt_entry_in	*ent_in = &lgc_seg->ls_ent_in;
	struct agg_phy_ent	*phy_ent;
	struct bio_io_context	*bio_ctxt;
	daos_iov_t		 iov;
	bio_addr_t		 addr_dst, addr_src;
	daos_size_t		 seg_size, copy_size;
	struct evt_extent	 ext = { 0 };
	daos_off_t		 phy_lo;
	unsigned int		 i;
	int			 rc;

	D_ASSERT(obj != NULL);
	D_ASSERT(mw->mw_rsize > 0);
	seg_size = evt_rect_width(&ent_in->ei_rect) * mw->mw_rsize;

	D_ASSERTF(seg_size > 0 && seg_size <= io->ic_buf_len,
		  "seg_size:"DF_U64" > buf_size:%u\n",
		  seg_size, io->ic_buf_len);

	phy_ent = lgc_seg->ls_phy_ent;
	if (phy_ent && bio_addr_is_hole(&phy_ent->pe_addr)) {
		bio_addr_set(&ent_in->ei_addr, DAOS_MEDIA_SCM, 0);
		bio_addr_set_hole(&ent_in->ei_addr, 1);

		return 0;
	}

	rc = reserve_segment(obj, io, seg_size, &ent_in->ei_addr);
	if (rc) {
		D_ERROR("Reserve "DF_U64" segment error: %d\n", seg_size, rc);
		return rc;
	}

	/* Copy data from old logical entries into new segment */
	D_ASSERT(lgc_seg->ls_idx_start <= lgc_seg->ls_idx_end);
	D_ASSERT(lgc_seg->ls_idx_end < mw->mw_lgc_cnt);

	bio_ctxt = obj->obj_cont->vc_pool->vp_io_ctxt;
	D_ASSERT(bio_ctxt != NULL);

	addr_dst = ent_in->ei_addr;
	D_ASSERT(!bio_addr_is_hole(&addr_dst));

	mark_yield(&addr_dst, acts);

	iov.iov_buf = io->ic_buf;
	iov.iov_buf_len = io->ic_buf_len;

	i = lgc_seg->ls_idx_start;
	while (ext.ex_hi < ent_in->ei_rect.rc_ex.ex_hi &&
	       i <= lgc_seg->ls_idx_end) {

		if (lgc_seg->ls_phy_ent != NULL) {
			phy_ent = lgc_seg->ls_phy_ent;
			ext = ent_in->ei_rect.rc_ex;
		} else {
			struct agg_lgc_ent *lgc_ent = &mw->mw_lgc_ents[i];

			phy_ent = lgc_ent->le_phy_ent;
			ext = lgc_ent->le_ext;
			i++;
		}
		D_ASSERT(ext1_covers_ext2(&ent_in->ei_rect.rc_ex, &ext));
		D_ASSERT(ext1_covers_ext2(&phy_ent->pe_rect.rc_ex, &ext));

		phy_lo = phy_ent->pe_rect.rc_ex.ex_lo;
		if (phy_ent->pe_off != 0)
			phy_lo += phy_ent->pe_off;

		D_ASSERT(phy_lo <= phy_ent->pe_rect.rc_ex.ex_hi);
		D_ASSERT(ext.ex_lo >= phy_lo);

		copy_size = evt_extent_width(&ext) * ent_in->ei_inob;

		addr_src = phy_ent->pe_addr;
		addr_src.ba_off += (ext.ex_lo - phy_lo);

		D_ASSERT(!bio_addr_is_hole(&addr_src));
		D_ASSERT(iov.iov_buf_len >= copy_size);

		mark_yield(&addr_src, acts);
		/*
		 * Set 'iov_len' beforehand, cause it will be used as copy
		 * size in bio_readv().
		 */
		iov.iov_len = copy_size;
		rc = bio_readv(bio_ctxt, addr_src, &iov);
		if (rc) {
			D_ERROR("Read "DF_RECT" "DF_EXT" error: %d\n",
				DP_RECT(&phy_ent->pe_rect), DP_EXT(&ext), rc);
			break;
		}

		D_ASSERT(iov.iov_len == copy_size);
		rc = bio_writev(bio_ctxt, addr_dst, &iov);
		if (rc) {
			D_ERROR("Write "DF_RECT" "DF_EXT" error: %d\n",
				DP_RECT(&ent_in->ei_rect), DP_EXT(&ext), rc);
			break;
		}

		D_ASSERT(seg_size >= copy_size);
		seg_size -= copy_size;
		addr_dst.ba_off += copy_size;
	}

	/* Update payload addr for truncated physical entry */
	if (lgc_seg->ls_phy_ent != NULL) {
		phy_ent = lgc_seg->ls_phy_ent;
		phy_ent->pe_addr = ent_in->ei_addr;
	}

	return rc;
}

static inline daos_size_t
merge_window_size(struct agg_merge_window *mw)
{
	D_ASSERT(mw->mw_ext.ex_hi >= mw->mw_ext.ex_lo);
	D_ASSERT(mw->mw_rsize != 0);
	return evt_extent_width(&mw->mw_ext) * mw->mw_rsize;
}

static int
fill_segments(daos_handle_t ih, struct agg_merge_window *mw,
	      unsigned int *acts)
{
	struct agg_io_context	*io = &mw->mw_io_ctxt;
	struct agg_lgc_seg	*lgc_seg;
	unsigned int		 i;
	int			 rc = 0;

	if (io->ic_buf_len < merge_window_size(mw)) {
		void *buffer;

		D_REALLOC(buffer, io->ic_buf, merge_window_size(mw));
		if (buffer == NULL)
			return -DER_NOMEM;

		io->ic_buf = buffer;
		io->ic_buf_len = merge_window_size(mw);
	}

	if (io->ic_scm_max < io->ic_seg_cnt) {
		struct pobj_action *scm_exts;

		D_REALLOC(scm_exts, io->ic_scm_exts,
			  io->ic_seg_cnt * sizeof(*scm_exts));
		if (scm_exts == NULL)
			return -DER_NOMEM;

		io->ic_scm_exts = scm_exts;
		io->ic_scm_max = io->ic_seg_cnt;
	}

	for (i = 0; i < io->ic_seg_cnt; i++) {
		lgc_seg = &io->ic_segs[i];

		rc = fill_one_segment(ih, mw, lgc_seg, acts);
		if (rc) {
			D_ERROR("Fill seg %u-%u %p "DF_RECT" error: %d\n",
				lgc_seg->ls_idx_start, lgc_seg->ls_idx_end,
				lgc_seg->ls_phy_ent,
				DP_RECT(&lgc_seg->ls_ent_in.ei_rect), rc);
			break;
		}
	}

	return rc;
}

static int
insert_segments(daos_handle_t ih, struct agg_merge_window *mw,
		unsigned int *acts)
{
	struct vos_obj_iter	*oiter = vos_hdl2oiter(ih);
	struct vos_object	*obj = oiter->it_obj;
	struct agg_io_context	*io = &mw->mw_io_ctxt;
	struct agg_phy_ent	*phy_ent, *tmp;
	struct agg_lgc_ent	*lgc_ent;
	struct evt_rect		 rect;
	unsigned int		 i, truncated = 0;
	int			 rc;

	D_ASSERT(obj != NULL);
	rc = umem_tx_begin(vos_obj2umm(obj), NULL);
	if (rc)
		return rc;

	/* Publish SCM reservations */
	if (io->ic_scm_cnt) {
		rc = umem_tx_publish(vos_obj2umm(obj), io->ic_scm_exts,
				     io->ic_scm_cnt);
		io->ic_scm_cnt = 0;
		if (rc) {
			D_ERROR("Publish %u SCM extents error: %d\n",
				io->ic_scm_cnt, rc);
			goto abort;
		}
	}

	/* Adjust logical entry queue */
	for (i = 0; i < mw->mw_lgc_cnt; i++) {
		lgc_ent = &mw->mw_lgc_ents[i];
		phy_ent = lgc_ent->le_phy_ent;

		D_ASSERT(ext1_covers_ext2(&mw->mw_ext, &lgc_ent->le_ext));
		D_ASSERT(phy_ent->pe_ref > 0);
		phy_ent->pe_ref--;
	}
	mw->mw_lgc_cnt = 0;

	/* Remove old physical entries from EV tree */
	d_list_for_each_entry_safe(phy_ent, tmp, &mw->mw_phy_ents, pe_link) {
		rect = phy_ent->pe_rect;

		/* The physical entry was truncated on prev window flush */
		if (phy_ent->pe_off != 0)
			rect.rc_ex.ex_lo += phy_ent->pe_off;

		D_ASSERT(rect.rc_ex.ex_lo <= rect.rc_ex.ex_hi);
		/* Physical entry must be in window or straddle window end */
		D_ASSERT(rect.rc_ex.ex_lo >= mw->mw_ext.ex_lo);
		D_ASSERT(rect.rc_ex.ex_lo <= mw->mw_ext.ex_hi);

		mark_yield(&phy_ent->pe_addr, acts);

		rc = evt_delete(oiter->it_hdl, &rect, NULL);
		if (rc) {
			D_ERROR("Delete "DF_RECT" error: %d\n",
				DP_RECT(&rect), rc);
			goto abort;
		}

		/* Physical entry is in window */
		if (rect.rc_ex.ex_hi <= mw->mw_ext.ex_hi) {
			D_ASSERT(phy_ent->pe_ref == 0);
			d_list_del(&phy_ent->pe_link);
			D_FREE_PTR(phy_ent);
			D_ASSERT(mw->mw_phy_cnt > 0);
			mw->mw_phy_cnt--;
			continue;
		}

		/* Update extent start of truncated physical entry */
		rect.rc_ex.ex_lo = mw->mw_ext.ex_hi + 1;
		phy_ent->pe_off = rect.rc_ex.ex_lo -
				phy_ent->pe_rect.rc_ex.ex_lo;
		truncated++;

	}
	D_ASSERT(truncated == mw->mw_phy_cnt);

	/* Clear window size */
	mw->mw_ext.ex_lo = mw->mw_ext.ex_hi = 0;

	/* Insert new segments into EV tree */
	for (i = 0; i < io->ic_seg_cnt; i++) {
		struct evt_entry_in *ent_in = &io->ic_segs[i].ls_ent_in;

		rc = evt_insert(oiter->it_hdl, ent_in);
		if (rc) {
			D_ERROR("Insert segment "DF_RECT" error: %d\n",
				DP_RECT(&ent_in->ei_rect), rc);
			goto abort;
		}
	}

	/* Publish NVMe reservations */
	rc = vos_publish_blocks(obj, &io->ic_nvme_exts, true);
	if (rc)
		D_ERROR("Publish NVMe extents error: %d\n", rc);
abort:
	if (rc)
		rc = umem_tx_abort(vos_obj2umm(obj), rc);
	else
		rc = umem_tx_commit(vos_obj2umm(obj));

	return rc;
}

static void
cleanup_segments(daos_handle_t ih, struct agg_merge_window *mw, int rc)
{
	struct vos_obj_iter	*oiter = vos_hdl2oiter(ih);
	struct vos_object	*obj = oiter->it_obj;
	struct agg_io_context	*io = &mw->mw_io_ctxt;

	D_ASSERT(obj != NULL);
	if (rc) {
		if (io->ic_scm_cnt) {
			umem_cancel(vos_obj2umm(obj), io->ic_scm_exts,
				    io->ic_scm_cnt);
			io->ic_scm_cnt = 0;
		}
		if (!d_list_empty(&io->ic_nvme_exts))
			vos_publish_blocks(obj, &io->ic_nvme_exts, false);
	}

	/* Reset io context */
	D_ASSERT(d_list_empty(&io->ic_nvme_exts));
	D_ASSERT(io->ic_scm_cnt == 0);
	io->ic_seg_cnt = 0;
}

static void
clear_merge_window(struct agg_merge_window *mw)
{
	struct agg_phy_ent *phy_ent, *tmp;

	mw->mw_ext.ex_lo = mw->mw_ext.ex_hi = 0;
	mw->mw_lgc_cnt = 0;
	d_list_for_each_entry_safe(phy_ent, tmp, &mw->mw_phy_ents,
				   pe_link) {
		d_list_del(&phy_ent->pe_link);
		D_FREE_PTR(phy_ent);
	}
	mw->mw_phy_cnt = 0;

}

static int
flush_merge_window(daos_handle_t ih, struct agg_merge_window *mw,
		   unsigned int *acts)
{
	int	rc;

	D_ASSERT(merge_window_status(mw) == MW_NONEMPTY);

	/* Only one physical entry in window, nothing to merge */
	if (mw->mw_phy_cnt == 1) {
		D_ASSERT(mw->mw_lgc_cnt == 1);
		clear_merge_window(mw);

		return 0;
	}

	/* Prepare the new segments to be inserted */
	rc = prepare_segments(mw);
	if (rc) {
		D_ERROR("Prepare segments "DF_EXT" error: %d\n",
			DP_EXT(&mw->mw_ext), rc);
		goto out;
	}

	/* Transfer data from old logical records to reserved new segments */
	rc = fill_segments(ih, mw, acts);
	if (rc) {
		D_ERROR("Fill segments "DF_EXT" error: %d\n",
			DP_EXT(&mw->mw_ext), rc);
		goto out;
	}

	/* Replace the old logical records with new segments in EV tree */
	rc = insert_segments(ih, mw, acts);
	if (rc) {
		D_ERROR("Insert segments "DF_EXT" error: %d\n",
			DP_EXT(&mw->mw_ext), rc);
		goto out;
	}
out:
	cleanup_segments(ih, mw, rc);
	return rc;
}

static bool
trigger_flush(struct agg_merge_window *mw, struct evt_extent *lgc_ext,
	      bool visible)
{
	struct evt_extent *w_ext = &mw->mw_ext;

	D_ASSERT(w_ext->ex_lo <= lgc_ext->ex_lo);

	/* Don't trigger flush when incoming entry isn't visible */
	if (!visible)
		return false;

	/* Empty or closed merge window */
	if (merge_window_status(mw) == MW_CLOSED ||
	    merge_window_status(mw) == MW_EMPTY)
		return false;

	/*
	 * Window is formed by visible logical entries, must have no
	 * overlapping.
	 */
	D_ASSERTF(w_ext->ex_hi < lgc_ext->ex_lo,
		  "win:"DF_EXT", lgc_ent:"DF_EXT"\n",
		  DP_EXT(w_ext), DP_EXT(lgc_ext));

	/* Window is large enough */
	if (merge_window_size(mw) >= AGG_MW_THRESH_FLUSH)
		return true;

	/* Trigger flush when entry is disjoint with window */
	return !((w_ext->ex_hi + 1) == lgc_ext->ex_lo);
}

static struct agg_phy_ent *
enqueue_phy_ent(struct agg_merge_window *mw, struct evt_extent *phy_ext,
		daos_epoch_t epoch, bio_addr_t *addr, uint32_t ver)
{
	struct agg_phy_ent	*phy_ent;

	D_ALLOC_PTR(phy_ent);
	if (phy_ent == NULL)
		return NULL;

	phy_ent->pe_rect.rc_ex = *phy_ext;
	phy_ent->pe_rect.rc_epc = epoch;
	phy_ent->pe_addr = *addr;
	phy_ent->pe_off = 0;
	phy_ent->pe_ver = ver;
	phy_ent->pe_ref = 0;

	/* Sanity check */
	if (!d_list_empty(&mw->mw_phy_ents)) {
		struct agg_phy_ent *prev;

		D_ASSERT(mw->mw_phy_cnt != 0);
		prev = d_list_entry(mw->mw_phy_ents.prev, struct agg_phy_ent,
				    pe_link);
		D_ASSERTF(prev->pe_rect.rc_ex.ex_lo <= phy_ext->ex_lo,
			  "prev phy_ext: "DF_EXT", phy_ext: "DF_EXT"\n",
			  DP_EXT(&prev->pe_rect.rc_ex), DP_EXT(phy_ext));
	} else {
		D_ASSERT(mw->mw_phy_cnt == 0);
	}

	d_list_add_tail(&phy_ent->pe_link, &mw->mw_phy_ents);
	mw->mw_phy_cnt++;

	return phy_ent;
}

static int
enqueue_lgc_ent(struct agg_merge_window *mw, struct evt_extent *lgc_ext,
		struct agg_phy_ent *phy_ent)
{
	struct agg_lgc_ent	*lgc_ent;
	unsigned int		 max, cnt;

	max = mw->mw_lgc_max;
	cnt = mw->mw_lgc_cnt;
	/* Sanity check */
	if (cnt > 0) {
		D_ASSERT(mw->mw_ext.ex_hi + 1 == lgc_ext->ex_lo);

		lgc_ent = &mw->mw_lgc_ents[cnt - 1];
		D_ASSERTF(lgc_ext->ex_lo == lgc_ent->le_ext.ex_hi + 1,
			  "prev lgc_ext: "DF_EXT", lgc_ext: "DF_EXT"\n",
			  DP_EXT(&lgc_ent->le_ext), DP_EXT(lgc_ext));
	}

	if (cnt == max) {
		unsigned int new_max = max ? max * 2 : 10;

		D_REALLOC(lgc_ent, mw->mw_lgc_ents,
			  new_max * sizeof(*lgc_ent));
		if (lgc_ent == NULL)
			return -DER_NOMEM;

		mw->mw_lgc_max = new_max;
		mw->mw_lgc_ents = lgc_ent;
	}

	D_ASSERT(mw->mw_lgc_max > mw->mw_lgc_cnt);
	lgc_ent = &mw->mw_lgc_ents[cnt];
	lgc_ent->le_ext = *lgc_ext;
	phy_ent->pe_ref++;
	lgc_ent->le_phy_ent = phy_ent;
	mw->mw_lgc_cnt++;

	/*
	 * Extend window size. If the visible entry is a punched record, the
	 * window size could be very huge, but this is ok, because there won't
	 * be huge contiguous allocation on window flush, only lots of covered
	 * physical entries being deleted.
	 */
	if (mw->mw_lgc_cnt == 1)
		mw->mw_ext.ex_lo = lgc_ext->ex_lo;
	mw->mw_ext.ex_hi = lgc_ext->ex_hi;

	return 0;
}

static void
close_merge_window(struct agg_merge_window *mw, int rc)
{
	struct agg_io_context *io = &mw->mw_io_ctxt;

	if (rc)
		clear_merge_window(mw);

	mw->mw_rsize = 0;
	D_ASSERT(mw->mw_ext.ex_lo == 0 && mw->mw_ext.ex_hi == 0);

	D_ASSERT(mw->mw_lgc_cnt == 0);
	if (mw->mw_lgc_ents != NULL) {
		D_FREE(mw->mw_lgc_ents);
		mw->mw_lgc_max = 0;
	}
	D_ASSERT(d_list_empty(&mw->mw_phy_ents));
	D_ASSERT(mw->mw_phy_cnt == 0);

	if (io->ic_buf != NULL) {
		D_FREE(io->ic_buf);
		io->ic_buf_len = 0;
	}

	D_ASSERT(io->ic_seg_cnt == 0);
	if (io->ic_segs != NULL) {
		D_FREE(io->ic_segs);
		io->ic_seg_max = 0;
	}

	D_ASSERT(io->ic_scm_cnt == 0);
	if (io->ic_scm_exts != NULL) {
		D_FREE(io->ic_scm_exts);
		io->ic_scm_max = 0;
	}
	D_ASSERT(d_list_empty(&io->ic_nvme_exts));
}

static inline void
recx2ext(daos_recx_t *recx, struct evt_extent *ext)
{
	D_ASSERT(recx->rx_nr > 0);
	ext->ex_lo = recx->rx_idx;
	ext->ex_hi = recx->rx_idx + recx->rx_nr - 1;
}

static struct agg_phy_ent *
lookup_phy_ent(struct agg_merge_window *mw, struct evt_extent *phy_ext,
	       daos_epoch_t epoch)
{
	struct agg_phy_ent *phy_ent;

	d_list_for_each_entry_reverse(phy_ent, &mw->mw_phy_ents, pe_link) {
		/* Physical entry list is sorted by extent start */
		D_ASSERT(phy_ent->pe_rect.rc_ex.ex_lo <= phy_ext->ex_lo);
		if (phy_ent->pe_rect.rc_ex.ex_lo < phy_ext->ex_lo)
			break;

		if (phy_ent->pe_rect.rc_epc == epoch &&
		    phy_ent->pe_rect.rc_ex.ex_hi == phy_ext->ex_hi)
			return phy_ent;
	}

	return NULL;
}

static int
join_merge_window(daos_handle_t ih, struct agg_merge_window *mw,
		  vos_iter_entry_t *entry, unsigned int *acts)
{
	struct evt_extent	 phy_ext, lgc_ext;
	struct agg_phy_ent	*phy_ent;
	bool			 visible;
	int			 rc;

	recx2ext(&entry->ie_recx, &lgc_ext);
	recx2ext(&entry->ie_orig_recx, &phy_ext);
	D_ASSERT(ext1_covers_ext2(&phy_ext, &lgc_ext));

	visible = (entry->ie_recx_flags & VOS_IT_RECX_VISIBLE);

	if (trigger_flush(mw, &lgc_ext, visible)) {
		rc = flush_merge_window(ih, mw, acts);
		if (rc) {
			D_ERROR("Flush window "DF_EXT" error: %d\n",
				DP_EXT(&mw->mw_ext), rc);
			return rc;
		}
		D_ASSERT(merge_window_status(mw) == MW_EMPTY);
	}

	phy_ent = lookup_phy_ent(mw, &phy_ext, entry->ie_epoch);
	if (phy_ent == NULL) {
		D_ASSERT(phy_ext.ex_lo == lgc_ext.ex_lo);

		phy_ent = enqueue_phy_ent(mw, &phy_ext, entry->ie_epoch,
					  &entry->ie_biov.bi_addr,
					  entry->ie_ver);
		if (phy_ent == NULL) {
			rc = -DER_NOMEM;
			D_ERROR("Enqueue phy_ent win:"DF_EXT", ent:"DF_EXT" "
				"error: %d\n", DP_EXT(&mw->mw_ext),
				DP_EXT(&phy_ext), rc);
			return rc;
		}
	} else {
		D_ASSERT(phy_ext.ex_lo != lgc_ext.ex_lo);
	}

	/* Enqueue the visible logical entry only */
	if (visible) {
		rc = enqueue_lgc_ent(mw, &lgc_ext, phy_ent);
		if (rc) {
			D_ERROR("Enqueue lgc_ent win: "DF_EXT", ent:"DF_EXT" "
				"error: %d\n", DP_EXT(&mw->mw_ext),
				DP_EXT(&lgc_ext), rc);
			return rc;
		}
	} else {
		/* Fully covered physical entry must have been excluded */
		D_ASSERT(entry->ie_recx_flags & VOS_RECX_FLAG_PARTIAL);
	}

	/* Flush & close window on last entry */
	if (entry->ie_recx_flags & VOS_RECX_FLAG_LAST) {
		rc = flush_merge_window(ih, mw, acts);
		if (rc)
			D_ERROR("Flush window "DF_EXT" error: %d\n",
				DP_EXT(&mw->mw_ext), rc);
		close_merge_window(mw, rc);
	}

	return rc;
}

static int
vos_agg_ev(daos_handle_t ih, vos_iter_entry_t *entry,
	   struct vos_agg_param *agg_param, unsigned int *acts)
{
	struct vos_obj_iter	*oiter = vos_hdl2oiter(ih);
	struct agg_merge_window	*mw = &agg_param->ap_window;
	struct evt_extent	 phy_ext, lgc_ext;
	int			 rc;

	D_ASSERT(agg_param != NULL);
	D_ASSERT(acts != NULL);
	recx2ext(&entry->ie_recx, &lgc_ext);
	recx2ext(&entry->ie_orig_recx, &phy_ext);

	/* Discard */
	if (agg_param->ap_discard) {
		if (uuid_compare(entry->ie_cookie, agg_param->ap_cookie))
			return 0;

		D_ASSERT(phy_ext.ex_lo == lgc_ext.ex_lo);
		rc = agg_del_entry(ih, agg_param->ap_umm, entry, acts);
		if (rc) {
			D_ERROR("Delete EV entry "DF_EXT" error: %d\n",
				DP_EXT(&phy_ext), rc);
		} else if (vos_iter_empty(ih) == 1) {
			agg_param->ap_ev_tree_empty = 1;
			/* Trigger re-probe in akey iteration */
			*acts |= VOS_ITER_CB_YIELD;
		}
		return rc;
	}

	/* Aggregation */
	D_DEBUG(DB_EPC, "lgc_ext: "DF_EXT", phy_ext: "DF_EXT"\n",
		DP_EXT(&lgc_ext), DP_EXT(&phy_ext));

	/* Set record size for merge window */
	D_ASSERT(entry->ie_rsize != 0);
	if (entry->ie_rsize >= AGG_MW_THRESH_FLUSH) {
		D_CRIT("EV tree record size "DF_U64" is too big, "
		       "needs a larger flush threshold "DF_U64"\n",
		       entry->ie_rsize, AGG_MW_THRESH_FLUSH);
		return -DER_INVAL;
	}

	if (mw->mw_rsize == 0)
		mw->mw_rsize = entry->ie_rsize;
	D_ASSERT(mw->mw_rsize == entry->ie_rsize);

	/* Just delete the fully covered intact physical entry */
	if ((entry->ie_recx_flags & VOS_RECX_FLAG_COVERED) &&
	    !(entry->ie_recx_flags & VOS_RECX_FLAG_PARTIAL)) {
		struct evt_rect rect;

		D_ASSERTF(lgc_ext.ex_lo == phy_ext.ex_lo &&
			  lgc_ext.ex_hi == phy_ext.ex_hi,
			  ""DF_EXT" != "DF_EXT"\n",
			  DP_EXT(&lgc_ext), DP_EXT(&phy_ext));

		rect.rc_ex = phy_ext;
		rect.rc_epc = entry->ie_epoch;
		mark_yield(&entry->ie_biov.bi_addr, acts);

		rc = evt_delete(oiter->it_hdl, &rect, NULL);
		if (rc)
			D_ERROR("Delete EV entry "DF_RECT" error: %d\n",
				DP_RECT(&rect), rc);
	} else {
		rc = join_merge_window(ih, mw, entry, acts);
		if (rc)
			D_ERROR("Join window "DF_EXT"/"DF_EXT" error: %d\n",
				DP_EXT(&mw->mw_ext), DP_EXT(&phy_ext), rc);
	}

	if (rc)
		close_merge_window(mw, rc);

	return rc;
}

static int
vos_aggregate_cb(daos_handle_t ih, vos_iter_entry_t *entry,
		 vos_iter_type_t type, vos_iter_param_t *param,
		 void *cb_arg, unsigned int *acts)
{
	struct vos_agg_param	*agg_param = cb_arg;
	struct vos_container	*cont;
	int			 rc;

	switch (type) {
	case VOS_ITER_OBJ:
		rc = vos_agg_obj(ih, entry, agg_param, acts);
		break;
	case VOS_ITER_DKEY:
		rc = vos_agg_dkey(ih, entry, agg_param, acts);
		break;
	case VOS_ITER_AKEY:
		rc = vos_agg_akey(ih, entry, agg_param, acts);
		break;
	case VOS_ITER_SINGLE:
		rc = vos_agg_sv(ih, entry, agg_param, acts);
		break;
	case VOS_ITER_RECX:
		rc = vos_agg_ev(ih, entry, agg_param, acts);
		break;
	default:
		D_ASSERTF(false, "Invalid iter type\n");
		rc = -DER_INVAL;
		break;
	}

	if (rc < 0) {
		D_ERROR("VOS aggregation failed: %d\n", rc);
		return rc;
	}

	cont = vos_hdl2cont(param->ip_hdl);
	if (cont->vc_abort_aggregation) {
		D_DEBUG(DB_EPC, "VOS aggregation aborted\n");
		cont->vc_abort_aggregation = 0;
		cont->vc_in_aggregation = 0;
		return 1;
	}

	if (*acts & VOS_ITER_CB_YIELD)
		agg_param->ap_credits = 0;
	else
		agg_param->ap_credits++;

	if (agg_param->ap_credits > agg_param->ap_credits_max) {
		agg_param->ap_credits = 0;
		*acts |= VOS_ITER_CB_YIELD;
		bio_yield();
	}

	return 0;
}

static int
aggregate_enter(struct vos_container *cont, bool discard)
{
	if (cont->vc_in_aggregation) {
		D_ERROR(DF_CONT": Already in ggregation. discard:%d\n",
			DP_CONT(cont->vc_pool->vp_id, cont->vc_id), discard);

		/*
		 * The container will be eventually aggregated on next time
		 * when the aggregation being triggered by metadata server.
		 *
		 * TODO: This can be improved by tracking the new requested
		 * aggregation epoch range in vos_container, and start new
		 * aggregation immediately after current one is done.
		 */
		return -DER_BUSY;
	}

	cont->vc_in_aggregation = 1;
	return 0;
}

static void
aggregate_exit(struct vos_container *cont, bool discard)
{
	D_ASSERT(cont->vc_in_aggregation);
	cont->vc_in_aggregation = 0;
}

static void
merge_window_init(struct agg_merge_window *mw)
{
	struct agg_io_context	*io = &mw->mw_io_ctxt;

	memset(mw, 0, sizeof(*mw));
	D_INIT_LIST_HEAD(&mw->mw_phy_ents);
	D_INIT_LIST_HEAD(&io->ic_nvme_exts);
}

int
vos_aggregate(daos_handle_t coh, daos_epoch_range_t *epr)
{
	struct vos_container	*cont = vos_hdl2cont(coh);
	vos_iter_param_t	 iter_param = { 0 };
	struct vos_agg_param	 agg_param = { 0 };
	struct vos_iter_anchors	 anchors = { 0 };
	int			 rc;

	D_ASSERT(epr != NULL);
	D_ASSERTF(epr->epr_lo < epr->epr_hi && epr->epr_hi != DAOS_EPOCH_MAX,
		  "epr_lo:"DF_U64", epr_hi:"DF_U64"\n",
		  epr->epr_lo, epr->epr_hi);

	rc = aggregate_enter(cont, false);
	if (rc)
		return rc;

	/* Set iteration parameters */
	iter_param.ip_hdl = coh;
	iter_param.ip_epr = *epr;
	/*
	 * Iterate in epoch reserve order for SV tree, so that we can know for
	 * sure the first returned recx in SV tree has highest epoch and can't
	 * be aggregated.
	 */
	iter_param.ip_epc_expr = VOS_IT_EPC_RR;
	/* EV tree iterator returns all sorted logical rectangles */
	iter_param.ip_recx_flags = VOS_IT_RECX_VISIBLE | VOS_IT_RECX_COVERED;

	/* Set aggregation parameters */
	agg_param.ap_umm = &cont->vc_pool->vp_umm;
	agg_param.ap_credits_max = AGG_CREDITS_MAX;
	agg_param.ap_credits = 0;
	agg_param.ap_discard = false;
	merge_window_init(&agg_param.ap_window);

	rc = vos_iterate(&iter_param, VOS_ITER_OBJ, true, &anchors,
			 vos_aggregate_cb, &agg_param);
	if (rc != 0)
		goto exit;

	/*
	 * Update LAE, when aggregating for snapshot deletion, the
	 * @epr->epr_hi could be smaller than the LAE
	 */
	if (cont->vc_cont_df->cd_hae < epr->epr_hi)
		cont->vc_cont_df->cd_hae = epr->epr_hi;
exit:
	aggregate_exit(cont, false);

	if (merge_window_status(&agg_param.ap_window) != MW_CLOSED)
		D_ASSERTF(false, "Merge window resource leaked.\n");

	return rc;
}

int
vos_discard(daos_handle_t coh, daos_epoch_range_t *epr, uuid_t cookie)
{
	struct vos_container	*cont = vos_hdl2cont(coh);
	daos_epoch_t		 max_epoch;
	vos_iter_param_t	 iter_param = { 0 };
	struct vos_agg_param	 agg_param = { 0 };
	struct vos_iter_anchors	 anchors = { 0 };
	int			 rc;

	D_ASSERT(epr != NULL);
	D_ASSERTF(epr->epr_lo <= epr->epr_hi,
		  "epr_lo:"DF_U64", epr_hi:"DF_U64"\n",
		  epr->epr_lo, epr->epr_hi);

	rc = aggregate_enter(cont, true);
	if (rc != 0)
		return rc;

	rc = vos_cookie_find_update(cont->vc_pool->vp_cookie_th, cookie,
				    epr->epr_lo, false, &max_epoch);
	if (rc) {
		if (rc == -DER_NONEXIST)
			rc = 0;
		goto exit;
	}

	D_DEBUG(DB_EPC, "Max epoch of "DF_UUID" is "DF_U64"\n",
		DP_UUID(cookie), max_epoch);

	/** If this is the max epoch skip discard */
	if (max_epoch < epr->epr_lo) {
		D_DEBUG(DB_EPC, "Max Epoch < epr_lo.. skip discard\n");
		goto exit;
	}

	/* Set iteration parameters */
	iter_param.ip_hdl = coh;
	iter_param.ip_epr = *epr;
	if (epr->epr_lo == epr->epr_hi)
		iter_param.ip_epc_expr = VOS_IT_EPC_EQ;
	else if (epr->epr_hi != DAOS_EPOCH_MAX)
		iter_param.ip_epc_expr = VOS_IT_EPC_RR;
	else
		iter_param.ip_epc_expr = VOS_IT_EPC_GE;
	/* EV tree iterator returns all unsorted physical rectangles */
	iter_param.ip_recx_flags = VOS_IT_RECX_ALL;

	/* Set aggregation parameters */
	agg_param.ap_umm = &cont->vc_pool->vp_umm;
	uuid_copy(agg_param.ap_cookie, cookie);
	agg_param.ap_credits_max = AGG_CREDITS_MAX;
	agg_param.ap_credits = 0;
	agg_param.ap_discard = true;

	rc = vos_iterate(&iter_param, VOS_ITER_OBJ, true, &anchors,
			 vos_aggregate_cb, &agg_param);
exit:
	aggregate_exit(cont, true);
	return rc;
}
